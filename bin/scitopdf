#!/bin/bash
#

_RED=$(tput setaf 1)
_GREEN=$(tput setaf 2)
_YELLOW=$(tput setaf 3)
_BLUE=$(tput setaf 4)
_MAGENTA=$(tput setaf 5)
_CYAN=$(tput setaf 6)
_WHITE=$(tput setaf 7)
_RESET=$(tput sgr0)
_BOLD=$(tput bold)

echo "${_GREEN} ░▄▀▀░▄▀▀░█░▀█▀░▄▀▄▒█▀▄░█▀▄▒█▀ "
echo          " ▒▄██░▀▄▄░█░▒█▒░▀▄▀░█▀▒▒█▄▀░█▀ ${_RESET}"

# Check arguments
args=("$@")
ELEMENTS=${#args[@]} # arguments number
for (( i=0;i<$ELEMENTS;i++)); do
	[[ $(echo ${args[${i}]}) == "-l" || \
		$(echo ${args[${i}]}) == "--list" ]] && \
		listing=true && \
		index_bibliography_file=$(( i + 1 )) && \
		bibliography_file=${args[${index_bibliography_file}]}
done

# Downloads folder [change that to your favorite location]
if [ -z "$destination" ]; then
	if [ -n "$XDG_DOWNLOAD_DIR" ]; then
		destination="$XDG_DOWNLOAD_DIR/scitopdf"
	else
		destination="$HOME/Downloads/scitopdf"
	fi
fi

[ -d $destination ] || mkdir -p "$destination" &> /dev/null || \
	echo -e "${_BLUE}Can't create downloads directory to $destination.\nPlease be sure you have permissions.\nPapers will not be stored.${_RESET}"

# Quickly check internet connection
if [[ $OSTYPE == 'darwin'* ]]; then
	[[ -z $(ping -q -t1 -c1 1.1.1.1) ]] && \
		read -p "No internet connection. Continue ? [y/N] " no_net_choice && \
		[[ $no_net_choice == "" || \
			! $(echo $no_net_choice | grep -io "y") == "y" ]] && exit 1
else
	[[ -z $(ping -q -w1 -c1 1.1.1.1) ]] && \
		read -p "No internet connection. Continue ? [y/N] " no_net_choice && \
		[[ $no_net_choice == "" || \
			! $(echo $no_net_choice | grep -io "y") == "y" ]] && exit 1
fi

# Check $READER variable (pdf reader)
[[ -v READER ]] || \
	if [[ $(command -v zathura &>/dev/null) ]]; then
		echo -e "No PDF reader.\nTry setting a \$READER variable, or install zathura."
	else
		READER=zathura
	fi

# Open PDF automatically after download
automatically_open="yes"

# Temporary files
# use different headers avoid(ish)ing blocation
headers_file="/tmp/scitopdf_headers"
[ ! -f "$headers_file" ] && touch "$headers_file"
# store SH address here
sci_address="/tmp/sci_address"
# store curl results here
scitopdf_curl="/tmp/scitopdf_curl"

# Timeout for Crossref : seems useless right now (03/2022)
# requests_timeout=6

##############
# Functions  #
##############
	#_________________#
	# Rolling headers #
	#-----------------#
change_headers() {
	cat "$headers_file" | sed '1q' >> "$headers_file"
	sed -i '1d' "$headers_file"
	if [[ $(cat "$headers_file") == "" ]]; then
		echo "Mozilla/5.0 (x11; ubuntu; linux x86_64; rv:59.0) gecko/20100101 firefox/59.0
	Mozilla/5.0 (iPad; CPU OS 8_1_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12B466 Safari/600.1.4
	Mozilla/5.0 (Linux; U; Android 4.0.3; en-us; KFTT Build/IML74K) AppleWebKit/537.36 (KHTML, like Gecko) Silk/3.68 like Chrome/39.0.2171.93 Safari/537.36
	Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; rv:11.0) like Gecko
	Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:40.0) Gecko/20100101 Firefox/40.0
	Mozilla/5.0 (X11; Linux x86_64; rv:34.0) Gecko/20100101 Firefox/34.0
	Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0
	Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.1) Gecko/2008070208 Firefox/3.0.1
	Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53 BingPreview/1.0b
	Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; MAARJS; rv:11.0) like Gecko
	Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0; SLCC2; .NET CLR 2.0.50727; .NET4.0C; .NET4.0E)
	Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36" | sed 's/\t//' >> "$headers_file"
	fi
	headers=$(echo $(cat "$headers_file" | head -n 1))
}
	#______________#
	# Find Sci-Hub #
	#--------------#
locate_website() {
	[[ -f "$sci_address" ]] && [[ ! $(cat "$sci_address") == "" ]] && site=$(cat "$sci_address") && return
	echo "${_BOLD}Looking for website...${_RESET}"
	site=$(curl -s -LH "" "https://sci-hub.now.sh/" | grep -i https://sci-hub... | grep -i biglink | grep -io https://sci-hub... | head -n 1)
	if [[ $(echo $(curl -v $site 2>&1) | grep -io "Could not resolve" | tail -n 1) = $(echo "Could not resolve") ]]; then
		# TODO store known stable addresses
		echo -e "Sci-Hub not found. \nCheck https://sci-hub.now.sh\nFor manual website setting, use the -u option" # TODO
		exit 1;
	else
		echo $site > "$sci_address"
	fi
}
	#__________________________#
	# Look for DOI on Crossref #
	#--------------------------#
doi_search() {
change_headers
if ! [[ $(echo $doi) = "" ]]; then
	if [[ $(echo $doi | grep -io "doi:") = "doi:" || $(echo $doi | grep -io "doi\.") = "doi." ]]; then
		doi=$(echo $doi | sed 's/http:\/\///' | grep -io "/.*" | sed 's/\///')
		echo "${_BOLD}DOI: ${_RESET}$doi"
	fi
	echo "${_BOLD}DOI: ${_RESET}$doi"
else
	echo "${_RED}DOI not found.${_RESET}"
	[[ ! -v listing ]] && exit 1;
fi
}
	#________________#
	# Look for paper #
	#----------------#
paper_search() {
if ! [[ $(echo "$concatenate") = "" ]]; then
	change_headers
	user_search=$(echo "$concatenate" | sed "s/'/\ / ; s/\ /+/g" | iconv -f utf8 -t ascii//TRANSLIT)
	[[ -v listing ]] && echo -e "\n${_BOLD}[Reference $ref_index]${_RESET} ${_RED}$line${_RESET}"
	if [[ $(echo "$user_search" | grep -o "$site") = "$site" ]]; then
		download_link=$(curl -s "$user_search" | grep -io "http.*\.pdf" | head -n 1)
		if [[ $download_link = "" ]]; then
			download_link=$(echo https:)$(curl -s "$user_search" | grep -io "\/\/.*\.pdf" | head -n 1)
		fi

	# if user search contains "/" it means it's either a DOI or an address, so immediately try on sci-hub :
	elif [[ $(echo $user_search | grep -o "\/") = "/" ]]; then
		download_link=$(curl -s "$site/$user_search" | grep -io "http.*\.pdf" | head -n 1)
		if [[ $download_link = "" ]]; then
			download_link=$(echo https:)$(curl -s "$site/$user_search" | grep -io "\/\/.*\.pdf" | head -n 1)
			if [[ $download_link = "https:" ]]; then
				echo "${_RED}Empty download link. Trying address with Crossref.${_RESET}"
				doi_search
				download_link=$(curl -s "$site/$doi" | grep -io "http.*\.pdf" | head -n 1)
				if [[ $download_link = "" ]]; then
					download_link=$(echo https:)$(curl -s "$site/$doi" | grep -io "\/\/.*\.pdf" | head -n 1)
				fi
			fi
		fi

	# else : search on crossref
	elif [[ $(echo $(curl -A "$headers" -s "https://search.crossref.org/?q=$user_search&from_ui=yes" | tee "$scitopdf_curl".txt) | grep -io "$user_search" | head -n 1) = "$user_search" ]]; then
		doi=$(cat "$scitopdf_curl".txt | grep -io "https://doi.*" | grep -io "doi.*" | sed 's/http:\/\///' | grep -io "/.*" | sed 's/\///' | head -n 2 | tail -n 1)
		doi_search
		download_link=$(curl -s "$site/$doi" | grep -io "http.*\.pdf" | head -n 1)
		if [[ $download_link = "" ]]; then
			download_link=$(echo https:)$(curl -s "$site/$doi" | grep -io "\/\/.*\.pdf" | head -n 1)
		fi

	# last try => download first paper found on crossref / mismatch risk
	elif [[ -f "$scitopdf_curl".txt && ! $(cat "$scitopdf_curl".txt) == "" ]]; then
		doi=$(cat "$scitopdf_curl".txt | grep -io "https://doi.*" | grep -io "doi.*" | sed 's/http:\/\///' | grep -io "/.*" | sed 's/\///' | head -n 2 | tail -n 1)
		doi_search
		download_link=$(curl -s "$site/$doi" | grep -io "http.*\.pdf" | head -n 1)
		if [[ $download_link = "" ]]; then
			download_link=$(echo https:)$(curl -s "$site/$doi" | grep -io "\/\/.*\.pdf" | head -n 1)
		fi

	fi

	if [[ $download_link = "" || $download_link = "https:" ]]; then
		echo "${_RED}Empty download link.${_RESET}"
		rm "$scitopdf_curl".txt &>/dev/null
		unset download_link
		[[ -v listing ]] && not_found=$(( $not_found + 1 ))
		return
	fi
	filename=$(echo $download_link | sed 's:.*/::')
	cd $destination
	if [[ $(echo $filename) == "" ]]; then
		if [[ $(echo $(curl -s $site/$doi) | grep -io "article not found") = "article not found" ]]; then
			echo "${_YELLOW}Paper not available on Sci-Hub.${_RESET}"
		else
			echo "${_YELLOW}Paper not found.\\nPlease check if your country is not blocking access to Sci-Hub. Use a VPN if possible.${_RESET}"
			[[ ! -v listing ]] && exit 1;
		fi
	else
		echo "${_BOLD}Downloading ...${_RESET}"
		curl "$download_link" --output "$filename" --progress-bar
		echo "${_GREEN}Done!${_RESET}"
		rm "$scitopdf_curl".txt &>/dev/null
		unset download_link
		if [ $(echo $automatically_open) = "yes" ]; then
			setsid $READER "$destination/$filename"
		fi
#		exit 1;
	fi
else
	echo "${_BOLD}Paper to search ${_RESET}[title, author, year, DOI, journal, URL...] : "
	read concatenate
	[[ $concatenate == "" ]] || paper_search
fi
}

get_bibliography() {
	automatically_open="no"
	locate_website
	ref_index=0
	not_found=0
	while read line; do concatenate=$(echo $line);
		if [[ "$concatenate" == "" ]]; then
			continue
		fi
		ref_index=$(( ref_index + 1 )); paper_search "$line"; done < $bibliography_file
	echo -e "${_YELLOW}\n>> End of file \"$bibliography_file\"${_RESET}"
	[[ ! $not_found == 0 ]] && echo -e "${_YELLOW}$not_found out of $ref_index references not found.${_RESET}" # TODO list not found references
	[[ $not_found == 0 ]] && echo -e "${_YELLOW}$ref_index out of $ref_index references found !${_RESET}"
	exit 1

}

################
# Start script #
################
if [[ $listing == true ]]; then
	get_bibliography
else
	# If no flag specified, concatenate given arguments for a single paper search.
	args=("$@")
	ELEMENTS=${#args[@]}
	for (( i=0;i<$ELEMENTS;i++)); do
		concatenate=$(echo $concatenate" ")$(echo -n ${args[${i}]}"\ ")
	done
fi

if [[ $(echo $concatenate | grep -io "http:") == "http:" ]]; then
	locate_website
	concatenate=$(echo $concatenate | sed "s/'//g ; s/\ //g" | sed 's/\\//g')
	download_link=$(echo $(curl -s $site/$concatenate) | grep -io "http.*\.pdf" | head -n 1)
	if [[ $(echo $download_link | cut -c1-2) == "//" ]]; then
		download_link=$(echo $download_link | sed 's/\/\//https:\/\//g')
	fi
	filename=$(echo $download_link | sed 's:.*/::')
	cd $destination
	if [[ $(echo $filename) == "" ]]; then
		echo -e "Paper not found. \nPlease be sure your country is not blocking access to Sci-Hub. \nUse a VPN if possible."
		[[ ! -v listing ]] && exit;
	else
		echo "${_BOLD}Downloading ...${_RESET}"
		curl "$download_link" --output "$filename" --progress-bar
		echo "${_GREEN}Done!${_RESET}"
		rm "$scitopdf_curl".txt &>/dev/null
		unset download_link
		if [ $(echo $automatically_open) = "yes" ]; then
			setsid $READER "$destination/$filename"
		fi
	fi
else
	concatenate=$(echo $concatenate | sed 's/\\//g')
	locate_website
	site=$(echo $(cat "$sci_address"))
	paper_search
fi


exit

# last modif : 2022 june 18
# add flags : -q = no echo ; --download-only = no auto-opening ; -d --doi ; -u = url
# make code proper/simpler : lot of redundancies
